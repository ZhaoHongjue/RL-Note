{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Markov决策过程\n",
    "\n",
    "## Markov决策过程模型\n",
    "\n",
    "### 离散时间Markov决策过程\n",
    "\n",
    "> 在离散时间智能体/环境接口中，智能体与环境交互的时刻为{0,1,2,3,...}。在时刻t，发生如下事情：\n",
    "> * 智能体观测时间得$O_t$\n",
    "> * 智能体根据观测环境决定做出动作$A_t$\n",
    "> * 环境根据智能体动作，给予智能体奖励$R_{t+1}$，并进入下一步状态$S_{t+1}$\n",
    "\n",
    "由此得：\n",
    "* S：状态空间，状态取值的综合\n",
    "* O：观测空间，观测取值的集合\n",
    "* A：动作集合\n",
    "  * 为了分析方便，往往采用一个包括所有可能动作的更大集合，使得每一步动作集合在数学上以同样的字母表示\n",
    "* R：奖励空间，是实数集的子集\n",
    "\n",
    "<u>离散时间不一定是间隔相同或间隔预先设定好的时间，这里的离散时间指标t只是表示决策和动作的指标</u>\n",
    "\n",
    "【**轨道**】$S_0,O_0,A_0,R_1,S_1,O_1,A_1,R_2,...$\n",
    "\n",
    "对于回合制任务，可能存在终止状态$s_{终止}$。\n",
    "* 达到终止状态时，回合结束，不再有任何观测或动作\n",
    "* *状态空间S中不包括终止状态*\n",
    "* 强调含有终止状态的状态空间为$S^+$\n",
    "* 回合制任务的轨道形式：$S_0,O_0,A_0,R_1,S_1,O_1,A_1,R_2,...,S_T = s_{终止}$\n",
    "* 回合制度任务中一个回合的步数$T$：随机变量，随机过程中可以看做一个停时\n",
    "\n",
    "> 部分不完全可观测问题：建模为部分可观测的Markov决策过程（POMDP）进行求解\n",
    "\n",
    "【Markov决策过程模型】\n",
    "* **概率**：在时间t，从状态$S_t = s$和动作$A_t = a$跳转到下一状态$S_{t+1} = s'$和动作$R_{t+1} = r$的概率为：$Pr[S_{t+1} = s',R_{t+1} = r|S_t = s,A_t = a]$\n",
    "* **Markov性**：认为奖励$R_{t+1}$和下一状态$S_{t+1}$仅仅依赖于当前状态$S_t$和动作$A_t$，而不依赖于更早的状态和动作\n",
    "  * 对状态的额外约束，状态必须含义可能对未来产生影响的所有过去信息\n",
    "\n",
    "> 智能体/环境接口没有假设状态满足Markov性。Markov性是满足Markov决策过程的特点\n",
    "> 可以从不满足Markov性的观测中构造满足Markov性的状态，或去学习Markov性"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 环境与动力\n",
    "\n",
    "对于有限Markov过程，可定义：\n",
    "* **动力**：$p(s',r|s,a) = Pr[S_{t+1} = s',R_{t+1} = r|S_t = s,A_t = a]$\n",
    "* **状态转移概率**：$p(s'|s,a) = Pr[S_{t+1} = s'|S_t = s,A_t = a] = \\sum_{r \\in R} p(s',r|s,a)$\n",
    "* **给定“状态-动作”的期望奖励**：$ r(s,a) = E[R_{t+1} | S_t = s,A_t = a] = \\sum_{r \\in R} r \\sum_{s' \\in S} p(s',r|s,a)$\n",
    "* **给定“状态-动作-下一状态”的期望奖励**：$ r(s,a,s') = E[R_{t+1} | S_t = s,A_t = a,S_{t+1} = s'] = \\sum_{r \\in R} r \\frac{p(s',r|s,a)}{p(s'|s,a)} $\n",
    "\n",
    "注：$E[R_{t+1} | S_t = s,A_t = a,S_{t+1} = s'] = \\sum_{r \\in R} r p(r|s,s',a) = \\sum_{r \\in R} r \\frac{p(r,s,s',a)}{p(s,s',a)} = \\sum_{r \\in R} r \\frac{p(s',r|s,a)}{p(s'|s,a)}$\n",
    "\n",
    "非有限Markov决策过程：定义时使用概率分布函数"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 智能体与策略\n",
    "\n",
    "【**策略**】：从状态到动作的转移概率\n",
    "\n",
    "对于有限Markov决策过程：$\\pi: S \\times A \\to [0,1]$可定义为：$\\pi(a|s) = Pr[A_t=a|S_t=s],s \\in S, a\\in A$\n",
    "\n",
    "动作集为连续，则采用概率分布进行定义\n",
    "\n",
    "【**确定性策略**】若某个策略$\\pi$对状态集内任意状态，均有$a \\in A$满足$\\pi(a'|s) = 0, a' \\ne a$，则称此策略为**确定性策略**。\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 奖励、回报和价值函数\n",
    "\n",
    "【**回报**】\n",
    "* 对于*回合制任务*，假设某一回合在第T步达到终止状态，则从步骤t以后的回报$G_t$可定义为未来奖励之和：$G_t = R_{t+1} + R_{t+2} +\\dots +R_{T} $\n",
    "* t为确定性变量，T为随机变量，因此在$G_t$定义中，不仅每一项是随机变量，而且含有的项数也是随机变量\n",
    "* 对*连续性任务*，若只是简单求和，会趋于无穷大。由此引入**折扣**，此时定义回报：$G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} +\\dots = \\sum_{\\tau = 0}^{+\\infty}\\gamma^\\tau R_{t+\\tau+1} $\n",
    "  其中$\\gamma \\in [0,1]$\n",
    "* 折扣因子决定了如何在最近的奖励和未来的奖励之间进行折中。未来$\\tau$步后得到的1单位奖励相当于现在的$\\gamma^\\tau$单位奖励。\n",
    "  * $\\tau = 0$：只考虑眼前利益，无视未来利益——**贪心算法**\n",
    "  * $\\tau = 1$：未来1单位利益和眼前1单位利益同等重要\n",
    "  一般设定$\\gamma \\in (0,1)$，若未来每一步奖励有界，则回报也有界。\n",
    "* 也可为*连续性任务*定义**平均奖励**：$\\bar{R} = \\lim_{t \\to \\infty} E[\\frac1t \\sum_{\\tau=1}^t R_\\tau]$\n",
    "\n",
    "【**状态价值函数**$v_\\pi(s)$】\n",
    "* 从状态s开始采用策略$\\pi$的期望回报\n",
    "* $v_\\pi(s) = E_\\pi[G_t|S_t = s]$\n",
    "\n",
    "【**动作价值函数**$q_\\pi(s,a)$】\n",
    "* 状态s采用动作a后，采用策略$\\pi$的期望回报\n",
    "* $q_\\pi(s,a) = E_\\pi[G_t|S_t=s,A_t=a]$\n",
    "\n",
    "对于终止状态：$v_pi(s_{终止}) = 0$；$q_pi(s_{终止},a) = 0$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bellman期望方程\n",
    "\n",
    "【**策略评估**】试图求解给定策略的价值函数。Bellman方程常用于策略评估\n",
    "\n",
    "【**状态价值和动作价值的互相表示**】\n",
    "* 用t时刻的动作价值函数表示t时刻的状态价值函数：$v_{pi} (s) = \\sum_a \\pi(a|s)q_\\pi(s,a), s \\in S $，推导略。\n",
    "  * 空心圆圈代表状态，实心圆圈代表状态动作对，则动作价值函数表示状态价值函数的过程可用**备份图**表示\n",
    "* 用t+1时刻的状态价值函数表示t时刻的动作价值函数：$q_\\pi(s,a) = \\sum_{s',r}p(s',r|s,a)[r+\\gamma v_\\pi(s'')]= r(s,a) + \\gamma\\sum_{s'} p(s'|s,a)v_\\pi(s'),s \\in S, a \\in A $\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "plaintext"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}