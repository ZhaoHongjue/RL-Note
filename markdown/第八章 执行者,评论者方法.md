# 执行者/评论者方法

本章介绍**带自益的策略梯度方法** 。此类算法将策略梯度和自益结合：

- 一方面，**用一个含参函数近似价值函数** ，然后用此价值函数的近似值估计回报值；

- 另一方面，利用**估计得到的回报值估计策略梯度** ，进而更新参数。

这两方面被称为**评论者** （critic）和**执行者** （actor）。所以带自益的策略梯度算法被称为执行者/评论者算法。

## 同策执行者/评论者算法

执行者/评论者方法同样采用含参函数$h(s, a; \theta) $表示偏好，用其softmax运算的结果$\pi(s|a;\theta)$来近似最优策略。

在更新参数$\theta$时，执行者/评论者方法依然依据策略梯度定理，取$E[\Psi_t\nabla \ln\pi(A_t|S_t;\theta)]$​为梯度方向进行迭代更新。其中，$\Psi_t = \gamma^t (G_t - B(s))$。$\Psi_t$也可是以下形式：

* 动作价值：$\Psi_t = \gamma^t q_\pi(S_t,A_t)$
* 优势函数：$\Psi_t = \gamma^t[q_\pi(S_t,A_t) - v_\pi(S_t)]$
* 时序差分：$\Psi_t = \gamma^t [R_{t+1} + \gamma v_\pi(S_{t+1}) - v_\pi(S_{t})]$

在以上形式中，往往用价值函数估计回报，以上三种表示方式分别用$q_\pi(S_t,A_t), q_\pi(S_t,A_t), R_t + \gamma v_\pi(S_{t+1})$表示回报，后两者减去了基线以减小方差。

在实际使用中，价值是不知道的。可以采取一定手段进行估计。可以用函数近似的方法，用含参函数$v(s;\bold{w})$或$q(s,a;\bold{w})$来近似$v_\pi$和$q_\pi$。

上一章中，带基线的简单策略梯度算法已经使用了含参函数$v(s;\bold{w})$作为基线函数。我们可以在此基础上进一步引入自益的思想，用价值的估计$U_t$代替$\Psi_t$表示回报部分。

> 只用采用了自益的方法，即用价值估计来估计回报，并引入了偏差，才是执行者/评论者算法。用价值估计来做基线并没有带来偏差（因为基线本身就可以任意选择），因此，带基线的简单策略梯度算法不是执行者/评论者算法

### 动作价值执行者/评论者算法

同策执行者/评论者算法在更新策略参数$\theta$时也应该试图减小$-\Psi_t\ln\pi(A_t|S_t;\theta)$，只是在计算$\Psi_t$时采用了基于自益的回报估计。以下算法给出了在估计回报为$q(S_t,A_t;w)$，并取$\Psi_t = \gamma^t q(S_t,A_t;w)$时的同策算法，称为动作价值执行者/评论者算法。

算法一开始初始化了策略参数和价值参数，虽然算法中写的是可以将这个参数初始化为任意值，但是若为神经网络参数则需要按照神经网络要求初始化参数。在迭代过程中有个变量$I$，用来存储策略梯度中的折扣因子$\gamma^t$。每一回合乘上$\gamma$就是$\gamma^t$

#### 算法实现

【输入】环境（无数学描述）

【输出】最优策略的参数估计$\pi(\theta)$

【参数】优化器（隐含学习率$\alpha^{(w)},\alpha^{(\theta)}$），折扣因子$\gamma$，控制回合数和回合内步数的参数

1. 初始化：$\theta \leftarrow any, w \leftarrow any$
2. 带自益的策略更新：对每个回合执行以下操作
   1. 初始化累积折扣：$I \leftarrow 1$
   2. 决定初始状态动作对：选择状态$S$并用$\pi(\cdot|S;\theta)$得到动作$A$
   3. 若回合未结束执行以下操作：
      1. 采样：根据$S,A$得$S',R$
      2. 执行：用$\pi(\cdot|S;\theta)$得到动作$A'$
      3. 估计回报：$U \leftarrow R+\gamma q(S',A';w)$
      4. 策略改进：更新$\theta$以减小$-Iq(S,A;w)\ln\pi(A|S;\theta)$（如$\theta \leftarrow \theta + \alpha^{(\theta)}Iq(S,A;w)\nabla\ln\pi(A|S;\theta)$）
      5. 更新价值：更新$w$以减小$[U - q(S,A;w)]^2$（如$w \leftarrow w + \alpha^{(w)}[U - q(S,A;w)]\nabla q(S,A;w)$）
      6. 更新累积折扣：$I \leftarrow \gamma I$
      7. 更新状态：$S \leftarrow S',A \leftarrow A'$

### 优势执行者/评论者算法

在基本执行者/评论者算法中引入基线函数$B(S_t) = v(S_t;w)$，就会得到$\Psi_t = \gamma^t[q(S_t,A_t;w) - v(S_t;w)]$，其中，$q(S_t,A_t;w) - v(S_t;w)$是优势函数的估计。

采用$q(S_t,A_t;w) - v(S_t;w)$进行优势函数估计，就需要搭建两个目标函数分别表示$q(w),v(w)$，此时采用$U_t = R_{t+1} + \gamma v(S_t;w)$，此时优势函数变为时序差分形式$R_{t+1} + \gamma v(S_{t+1}, w) - v(S_t;w)$

#### 算法实现

【输入】环境（无数学描述）

【输出】最优策略的参数估计$\pi(\theta)$

【参数】优化器（隐含学习率$\alpha^{(w)},\alpha^{(\theta)}$），折扣因子$\gamma$，控制回合数和回合内步数的参数

1. 初始化：$\theta \leftarrow any, w \leftarrow any$
2. 带自益的策略更新：对每个回合执行以下操作
   1. 初始化累积折扣：$I \leftarrow 1$
   2. 决定初始状态动作对：选择状态$S$并用$\pi(\cdot|S;\theta)$得到动作$A$
   3. 若回合未结束执行以下操作：
      1. 采样：根据$S,A$得$S',R$
      2. 执行：用$\pi(\cdot|S;\theta)$得到动作$A'$
      3. 估计回报：$U \leftarrow R+\gamma v(S';w)$
      4. 策略改进：更新$\theta$以减小$-I[U-v(S;w)]\ln\pi(A|S;\theta)$（如$\theta \leftarrow \theta + \alpha^{(\theta)}I[U-v(S;w)]\nabla\ln\pi(A|S;\theta)$）
      5. 更新价值：更新$w$以减小$[U - v(S;w)]^2$（如$w \leftarrow w + \alpha^{(w)}[U - v(S;w)]\nabla v(S;w)$）
      6. 更新累积折扣：$I \leftarrow \gamma I$
      7. 更新状态：$S \leftarrow S'$

---

 若优势执行者/评论者算法在执行过程中**不是每一步都更新参数**，而是**在回合结束后用整个轨迹来进行更新**，就可以把算法分为**经验搜集**和**经验使用**两个部分。这样的分割可以让此算法同时有很多执行者同时执行。*例如，让多个执行者同时分别收集很多经验，然后都用自己的那些经验得到一批经验所带来的梯度更新值。每个执行者在一定时机内更新参数，同时更新策略参数$\theta$和价值参数$w$。*每个执行者的更新是异步的，由此称为**异步优势执行者/评论者算法**（A3C）。异步优势执行者/评论者算法中的自益部分，不仅可采用单步时序差分，也可使用多步时序差分。另外，还可以对函数参数的访问进行控制，使得所有的执行者统一更新参数。这样的算法称为优势执行者/评论者算法。

#### 异步优势执行者/评论者算法实现（某个线程的行为）

【输入】环境（无数学描述）

【输出】最优策略的参数估计$\pi(\theta)$

【参数】优化器（隐含学习率$\alpha^{(w)},\alpha^{(\theta)}$），折扣因子$\gamma$，控制回合数和回合内步数的参数

1. 同步全局参数：$\theta' \leftarrow \theta, w' \leftarrow w$
2. 逐回合执行以下过程：
   1. 用策略$\pi(\theta')$生成轨迹，知道回合结束或达到上限$T$
   2. 为梯度计算初始化：
      1. 初始化目标$U_T$：若$S_T$是终止状态，则$U \leftarrow 0$；否则$U\leftarrow v(S_t;w')$
      2. 初始化梯度：$g^{(\theta)} \leftarrow 0, g^{(w)} \leftarrow 0$
   3. 异步计算梯度：对$t = T-1, T-2, \dots, 0$执行以下内容：
      1. 估计目标$U_t$：$U \leftarrow \gamma U + R_{t+1}$
      2. 估计策略梯度方向：$g^{(\theta)} \leftarrow g^{(\theta)} + [U - v(S_t;w')]\nabla \ln\pi(A_t|S_t;\theta')$
      3. 估计价值梯度方向：$g^{(w)} \leftarrow g^{(w)} + [U - v(S_t;w')]\nabla v(S_t;w')$
   4. 同步更新：更新全局参数
      1. 策略更新：用梯度方向$g^{(\theta)}$更新策略参数$\theta$（$\theta \leftarrow \theta + \alpha^{(\theta)}g^{(\theta)}$）
      2. 策略更新：用梯度方向$g^{(w)}$更新策略参数$\theta$（$w \leftarrow w + \alpha^{(w)}g^{(w)}$）

### 带资格迹的执行者/评论者算法

此算法中有两个资格迹$z^{(\theta)}$和$z^{(w)}$，它们分别与策略参数$\theta$，价值参数$w$对应，并可分别有自己的$\lambda^{(\theta)}$和$\lambda^{(w)}$。具体而言，$z^{(w)}$与价值参数$w$对应，运用梯度为$\nabla v(S;w)$，参数为$\lambda^{(w)}$的资格迹；$z^{(\theta)}$与价值参数$w$对应，运用梯度为$\nabla \ln \pi(A|S;w)$，参数为$\lambda^{(\theta)}$的资格迹，在运用中可以将折扣$\gamma^t$整合到资格迹中

#### 算法实现

【输入】环境（无数学描述）

【输出】最优策略的参数估计$\pi(\theta)$

【参数】资格迹参数$\lambda^{(\theta)}$，$\lambda^{(w)}$，优化器（隐含学习率$\alpha^{(w)},\alpha^{(\theta)}$），折扣因子$\gamma$，控制回合数和回合内步数的参数

1. 初始化：$\theta \leftarrow any, w \leftarrow any$

2. 带自益的策略更新：对每个回合执行以下操作：

   1. 初始化资格迹和累积折扣：$z^{(\theta)} \leftarrow 0, z^{(w)} \leftarrow 0, I \leftarrow 1$
   2. 决定初始状态：选择状态$S$
   3. 若回合未结束执行以下操作：
      1. 采样：用$\pi(\cdot|S;\theta)$得到动作$A$
      2. 执行：执行动作$A$，得到奖励$R$和观测$S'$
      3. 估计回报：$U \leftarrow R +\gamma v(S';w)$
      4. 更新策略资格迹：$z^{(\theta)} \leftarrow \gamma \lambda^{(\theta)}z^{(\theta)} + I \nabla\ln\pi(A|S;w)$
      5. 策略改进：$\theta \leftarrow \theta + \alpha^{(\theta)}[U - v(S;w)]z^{(\theta)}$
      6. 更新价值资格迹：$z^{(w)} \leftarrow \gamma \lambda^{(w)}z^{(w)} + \nabla v(S;w)$
      7. 更新价值：$w \leftarrow w + \alpha^{(w)}[U - v(S;w)]z^{w}$
      8. 更新累积折扣$I$：$I \leftarrow \gamma I$
      9. 更新状态$S \leftarrow S'$


## 基于代理优势的同策算法

* 迭代过程中不直接优化期望目标，而是试图优化期望目标的近似——代理优势

### 代理优势

【**性能差别引理**】$E_{\pi(\theta)}[G_0] = E_{\pi(\theta_k)}[G_0] + E_{\pi(\theta)}\left[ \sum_{t=0}^{+\infty}\gamma^t a_{\pi(\theta_k)}(S_t,A_t) \right]$（证明思路：化简$E_{\pi(\theta)}\left[ \sum_{t=0}^{+\infty}\gamma^t a_{\pi(\theta_k)}(S_t,A_t) \right]$）

* 要最大化$E_{\pi(\theta)}[G_0]$，就是要最大化优势期望$E_{\pi(\theta)}\left[ \sum_{t=0}^{+\infty}\gamma^t a_{\pi(\theta_k)}(S_t,A_t) \right]$

* 利用重采样将对$A_t\sim\pi(\theta)$求期望转化为对$A_t \sim \pi(\theta_k)$的期望：
  $$
  E_{S_t, A_t\sim\pi(\theta)}[a_\pi(S_t, A_t)] = E_{S_t\sim\pi(\theta), A_t\sim\pi(\theta_k)}\left[\frac{\pi(A_t|S_t;\theta)}{\pi(A_t|S_t;\theta_k)}a_\pi(S_t, A_t)\right]
  $$
  对$S_t\sim\pi(\theta)$求期望无法进一步转化，代理优势则在以上基础下，将对$S_t\sim\pi(\theta)$近似为对$S_t\sim\pi(\theta_k)$求期望：
  $$
  E_{S_t, A_t\sim\pi(\theta)}[a_\pi(S_t, A_t)] = E_{S_t, A_t\sim\pi(\theta_k)}\left[\frac{\pi(A_t|S_t;\theta)}{\pi(A_t|S_t;\theta_k)}a_\pi(S_t, A_t)\right]
  $$
  

