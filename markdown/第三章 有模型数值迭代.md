# 有模型数值迭代

假设动力系统完全已知的情况下，用迭代的数值方法来求解Bellman方程，得到价值函数与最优策略。

由于有模型迭代没有从数据里学习，所以<u>一般不认为是一种机器学习或强化学习方法</u>

## 度量空间与压缩映射

**有模型策略迭代的理论基础**：度量空间上的Banach不动点定理

### 度量空间及其完备性

【**度量**】定义在集合上的二元函数。对于集合$X$，其上度量$d:X \times X \to R$，需要满足：
* **非负性**：对于任意$x',x'' \in X$，有$d(x',x'') \ge 0$
* **同一性**：对于任意$x',x'' \in X$，若$d(x',x'') = 0$，$x' = x''$
* **对称性**：对于任意$x',x'' \in X$，有$d(x',x'') = d(x'',x')$
* **三角不等式**：对于任意$x',x'',x''' \in X$，有$d(x',x'') \le d(x',x''') + d(x'',x''') $

有序对$(X,d)$又称为**度量空间**

> 考虑有限Markov决策过程状态函数$v(s)$，其所有可能的取值组成集合$V \in R^{|S|}$，定义$d_{\infty}$：$d_{\infty} = \max_{s \in S} |v'(s) - v''(s)|$。由于满足非负性、同一性、对称性、三角不等式，所以$d_{\infty}$是$V$上的一个度量

【**完备性**】对于一个度量空间，若Cauchy序列都收敛在该空间内，则称这个度量空间是**完备的**

> 实数集就是著名的完备空间。

### 压缩映射与Bellman算子

【**压缩映射**】对于一个度量空间$(X,d)$和其上的一个映射$t:X \to X$，如果存在某个实数$\gamma \in (0,1)$，使得对于任意的$x',x''\in X$，都有：$d(t(x'),t(x'')) < \gamma d(x',x'')$，则称$t$为**压缩映射**。$\gamma$为Lipschitz常数。

【**Bellman最优算子**】
* 给定策略$\pi(a|s)$的**Bellman期望算子**$t_\pi:V \to V$：
  $$
  t_\pi(v)(s) = \sum_a \pi(a|s) \left[ r(s,a) + \gamma \sum_{s'} p(s'|s,a)v(s') \right]
  $$
  
* **Bellman最优算子**$t_*:V \to V$：
  $$
  t_*(v)(s) = \max_{a\in A} \left[ r(s,a) +\gamma \sum_{s' \in S} p(s'|s,a)v_*(s') \right]
  $$

这两个算子都是压缩映射。

### Banach不动点定理

【**不动点**】对于度量空间$(X,d)$上的映射$t:X \to X$，若$x \in X$使得$t(x) = x$，则称$x$是映射$t$的**不动点**

> 策略$\pi$的状态价值函数$v_\pi(s)$满足Bellman期望方程，是Bellman期望算子$t_\pi$算子的不动点。
>
> 最优状态价值$v_*(s)$满足Bellman最优方程，是Bellman最优算子$t_*$的不动点

【**Banach不动点定理**】

* 又称为压缩映射定理。$(X,d)$是非空的完备度量空间，$t:X \to X$是一个压缩映射，则映射$t$在$X$内有且仅有一个**不动点**。

* 此不动点可通过下列方法求出：从$X$内任意的一个元素$x_0$开始，定义迭代序列$x_k = t(x_{k-1})$，这个序列收敛，且极限为$x_{+\infty}$

* 从任意的起点开始，不断迭代使用压缩映射，就能得到不动点。迭代正比于$\gamma^k$的速度收敛。（$k$为迭代次数）
* **由此可以利用迭代法求得策略的价值或最优价值**。

## 有模型策略迭代

在给定动力系统$p$的情况下的策略评估、策略改进和策略迭代

* **策略评估**：对于给定策略$\pi$，<u>估计策略的价值，包括动作价值和状态价值</u>
* **策略改进**：对于给定策略$\pi$，在已知其价值函数的情况下<u>找到一个更优策略</u>
* **策略迭代**：综合利用策略评估和策略改进，找到最优策略

### 策略评估

#### 算法一

【输入】动力系统$p$，策略$\pi$

【输出】状态价值函数$v_\pi$的估计值

【参数】控制迭代次数的参数（如容忍误差度$\varepsilon_{\max}$或最大迭代次数$k_{\max}$）

1. **初始化**：对于$s \in S$，将$v_0(s)$初始化为任意值（比如0）。如果有终止状态，则将终止状态初始化为0

2. **迭代**：对于$k \leftarrow 1,2,3\dots$，迭代执行以下策略：

   1. 对于$s \in S$，逐一更新$v_{k+1}(s) \leftarrow \sum_{a} \pi(a|s)q_k(s,a)$，其中
      $$
      q_k(s,a) \leftarrow r(s,a) + \gamma \sum_{s'}p(s'|s,a)v_k(s')
      $$

   2. 若满足迭代终止条件，则跳出循环

      **常用终止条件**：1. 达到最大迭代次数$k_{\max}$；2. 满足最大误差限$\varepsilon_{\max}$达到精度要求

#### 算法二

【输入】动力系统$p$，策略$\pi$

【输出】状态价值函数$v_\pi$的估计值

【参数】控制迭代次数的参数（如容忍误差度$\varepsilon_{\max}$或最大迭代次数$k_{\max}$）

1. **初始化**：对于$s \in S$，将$v_0(s)$初始化为任意值（比如0）。如果有终止状态，则将终止状态初始化为0
2. **迭代**：对于$k \leftarrow 1,2,3\dots$，迭代执行以下策略：
   1. 计算新的状态价值$v_{新} \leftarrow \sum_{a} \pi(a|s)\left[ r(s,a) + \gamma \sum_{s'}p(s'|s,a)v(s') \right]$
   2. 对于使用误差限的情况，更新本次迭代观测到的最大误差$\varepsilon \leftarrow \max \{ \varepsilon | v_{新} - v(s) \}$
   3. 更新状态价值函数$v(s) \leftarrow v_{新}$
3. 满足迭代终止条件则退出循环

【**迭代策略评估算法的意义**】

1. 此策略评估算法将作为策略迭代算法的一部分，可用于最优策略的求解
2. 在此基础上修改可得到迭代求解最优策略的算法

