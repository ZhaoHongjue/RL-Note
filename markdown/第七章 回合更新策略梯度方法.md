# 第七章 回合更新策略梯度方法

* **最优价值算法**：利用价值函数，在求解最优策略过程中试图估计最优价值函数
  * 有模型价值迭代，回合更新，时序差分，函数近似
* **最优策略算法**：不直接估计最优价值函数，试图用含参函数近似最优策略，并通过迭代更新参数值

## 策略梯度算法原理

**核心思想**：1. 用含参函数近似最优策略；2. 用策略梯度优化策略函数

### 函数近似和动作偏好

【**基本思想**】用含参函数$\pi(a|s;\theta)$来近似最优策略。

> 由于$\sum_a \pi(s|a) = 1$，也应有$\sum_a \pi(a|s;\theta)=1$

【**动作偏好函数**】记为$h(a,s;\theta)$，其softmax的值为$\pi(a|s;\theta)$，即：
$$
\pi(a|s;\theta) = \frac{\exp h(a,s;\theta)}{\sum_{a'}\exp h(a',s;\theta)}
$$

* 从动作价值函数导出的最优策略往往具有固定形式（如$\varepsilon$贪心策略）。从动作偏好导出的最优策略估计不拘泥于特定形式，每个动作都可以有不同的概率值，形式更加灵活。
* 如果需要迭代方法更新参数$\theta$，随着迭代的进行，$\pi(a|s;\theta)$可以自然而然地逼近确定性策略，不需要手动调节参数
* 动作偏好函数可以具有线性组合、人工神经网络等多种形式，在确定动作偏好的形式后，只需要再确定参数$\theta$的值，就可以确定整个最优状态估计。参数$\theta$的值常常通过基于梯度的迭代算法更新。

### 策略梯度定理

策略梯度定理给出了<u>期望回报</u>和<u>策略梯度</u>之间的关系，是策略梯度方法的基础。

【**内容**】在回合制任务中，策略$\pi(\theta)$的期望回报可以表示为$E_{\pi(\theta)}$的期望回报可以表示为$E_{\pi(\theta)}[G_0]$。策略梯度定理给出了其对参数$\theta$的梯度为：
$$
\nabla E_{\pi(\theta)}[G_0] = E\left[ \sum_{t=0}^{+\infty}\gamma^t G_t \nabla \ln \pi(A_t|S_t;\theta) \right]
$$
等式的右边是和的期望，求和的$\gamma^t G_t \nabla \ln \pi(A_t|S_t;\theta)$中，只有$\nabla \ln \pi(A_t|S_t;\theta)$显式含有参数$\theta$

策略梯度定理告诉我们：只要知道$\nabla \ln \pi(A_t|S_t;\theta)$的值和其他一些比较好获得的值，就可以得到期望回报的梯度。这样就可以顺着梯度方向改变$\theta$以增大期望回报

证明暂略

## 同策回合更新策略梯度算法

### 简单的策略梯度算法

每个回合结束后，我们可以就回合中的每一步用形如
$$
\theta_{t+1} \leftarrow \theta + \alpha \gamma^t G_t \nabla \ln \pi(A_t|S_t;\theta)
$$

的迭代式来更新参数$\theta$，这样的算法称为**简单的策略梯度算法**（VPG）

$\alpha\gamma^t G_t \nabla \ln \pi(A_t|S_t;\theta)$：REward Increment = Nonnegtive Factor * Offset Reinforcement * Characteristic Eligibility表示增量

在具体更新过程中不一定要严格采取这种形式，当采用TensorFlow等自动微分的软件包来学习参数时，可以定义单步损失为$-\gamma^tG_t\ln \pi(A_t|S_t;\theta)$，让软件包中的优化器减小整个回合中所有步的平均损失。就会沿着$\sum_{t=0}^{+\infty} \gamma^t G_t \nabla \ln \pi(A_t|S_t;\theta)$的梯度方向改变$\theta$值

#### 算法实现

【输入】环境（无数学描述）

【输出】最优策略的估计$\pi(\theta)$

【参数】优化器（隐含学习率$\alpha$），折扣因子$\gamma$，控制回合数和步数的参数

1. 初始化：$\theta \leftarrow any$
2. 回合更新：对每个回合执行以下操作：
   1. 采样：用策略$\pi(\theta)$生成轨迹$S_0,A_0,R_1,S_1,\dots,S_{T-1},A_{T-1},R_T,S_T$
   2. 初始化回报：$G \leftarrow 0$
   3. 对$t = T-1,T-2,\dots,0$执行以下操作：
      1. 更新回报：$G \leftarrow \gamma G + R_{t+1}$
      2. 更新策略：更新$\theta$以减小$-\gamma^tG\ln\pi(A_t|S_t;\theta)$ （如$\theta_{t+1} \leftarrow \theta + \alpha \gamma^t G_t \nabla \ln \pi(A_t|S_t;\theta)$）

### 带基线的简单策略梯度算法

为了降低学习过程中的方差，可以引入基线函数$B(s)$。基线函数$B$可以是任意随机函数或确定函数，但是只能和状态$s$有关，不能和动作$a$有关。满足这样的条件后则会有：
$$
E[\gamma^t(G_t-B(S_t))\nabla\ln\pi(A_t|S_t;\theta)] = E[\gamma^tG_t\nabla\ln\pi(A_t|S_t;\theta)]
$$
证明略。基线函数可以任意选择，例如以下情况：

1. 选择基线函数为由轨迹确定的随机变量$B(S_t) = -\sum_{\tau=0}^{t-1} \gamma^{\tau-1}R_{\tau+1}$，这时$\gamma^t(G_t - B(S_t)) = G_0$，梯度形式为$E[G_0\nabla\ln\pi(A_t|S_t;\theta)]$
2. 选择基线函数为$B(S_t) = \gamma^tv_*(S_t)$，这时梯度形式为$E[\gamma^t(G_t-v_*(S_t))\nabla\ln\pi(A_t|S_t;\theta)]$

在选择基线时，应该参考如下两个思想：

1. 基线的选择应当有效降低方差。可通过实践得知
2. 基线函数应当是可以得到的。

一个能有效降低方差的基线是状态价值函数的估计，采用状态价值函数估计为基线的算法如下所示。这个算法有两套参数$\theta$和$w$，分别是最优策略估计和最优状态价值估计的参数，每次迭代时都以各自的算法案进行学习。

#### 算法实现

【输入】环境（无数学描述）

【输出】最优策略的估计$\pi(\theta)$

【参数】优化器（隐含学习率$\alpha^{(w)},\alpha^{(\theta)}$），折扣因子$\gamma$，控制回合数和步数的参数

1. 初始化：$\theta \leftarrow any,w \leftarrow any$
2. 回合更新：对每个回合执行以下操作：
   1. 采样：用策略$\pi(\theta)$生成轨迹$S_0,A_0,R_1,S_1,\dots,S_{T-1},A_{T-1},R_T,S_T$
   2. 初始化回报：$G \leftarrow 0$
   3. 对$t = T-1,T-2,\dots,0$执行以下操作：
      1. 更新回报：$G \leftarrow \gamma G + R_{t+1}$
      2. 更新价值：更新$w$以减小$[G - v(S_t;w)]^2$（如$w \leftarrow w + \alpha^{(w)}[G-v(S_t;w)]\nabla v(S_t;w)$）
      3. 更新策略：更新$\theta$以减小$-\gamma^tG\ln\pi(A_t|S_t;\theta)$ （如$\theta_{t+1} \leftarrow \theta + \alpha \gamma^t G_t \nabla \ln \pi(A_t|S_t;\theta)$）

基线可以最大程度减小方差的原因：考虑$E[\gamma^t(G-B(S_t))\nabla \ln\pi(A_t|S_t;\theta)]$的方差为：
$$
E\left[[\gamma^t(G-B(S_t))\nabla \ln\pi(A_t|S_t;\theta)]^2\right] - [E[\gamma^t(G-B(S_t))\nabla \ln\pi(A_t|S_t;\theta)]]^2
$$
对$B(S_t)$求偏导数：
$$
E[-2\gamma^{2t}(G_t - B(S_t))[\nabla\ln\pi(A_t|S_t;\theta)]^2]
$$
令此偏导数为0，并假设
$$
E[B(S_t)[\nabla \ln\pi(A_t|S_t;\theta)]] = E[B(S_t)]E[[\nabla\ln\pi(A_t|S_t;\theta)]^2]
$$
可知：
$$
E[B(S_t)] = \frac{E[G_t[\nabla\ln\pi(A_t|S_t;\theta)]^2]}{E[[\nabla \ln\pi(A_t|S_t;\theta)]^2]}
$$
这意味着，最佳基线函数应当接近回报$G_t$以梯度$[\nabla\ln\pi(A_t|S_t;\theta)]^2$为权重加权平均的结果。但是在实际应用中无法事先得知

当策略参数和价值参数同时需要学习时，算法的收敛性需要通过双时间轴Robbins-Monro算法来分析

## 异策回合更新策略梯度算法

引入重要性采样：
$$
\begin{aligned}
&\sum_a \pi(a|s;\theta)\gamma^tG_t\nabla\ln\pi(a|s;\theta) \\
&= \sum_ab(a|s)\frac{\pi(a|s;\theta)}{b(a|s)}\gamma^tG_t\nabla\ln\pi(a|s;\theta)\\
&=\sum_ab(a|s)\frac{1}{b(a|s)}\gamma^tG_t\nabla\pi(a|s;\theta)\\
\end{aligned}
$$
即
$$
E_{\pi(\theta)}[\gamma^tG_t\nabla\ln\pi(A_t|S_t;\theta)] = E_b\left[\frac{1}{b(A_t|S_t)}\gamma^tG_t\nabla\pi(A_t|S_t;\theta)\right]
$$
所以重要性采样的离线算法，只需要把用在在线策略采样得到的梯度方向$\gamma^tG_t\nabla\ln\pi(A_t|S_t;\theta)$改为用行为策略$b$采样得到的梯度方向$\frac{1}{b(A_t|S_t)}\gamma^tG_t\nabla\pi(A_t|S_t;\theta)$即可。这就意味着，在更新参数$\theta$时可以试图增大$\frac{1}{b(A_t|S_t)}\gamma^tG_t\pi(A_t|S_t;\theta)$

#### 算法实现

1. 初始化：$\theta \leftarrow any$
2. 回合更新：对每个回合执行以下操作：
   1. 行为策略：指定行为策略，使得$\pi(\theta) \ll b$
   2. 采样：用策略$b$生成轨迹$S_0,A_0,R_1,S_1,\dots,S_{T-1},A_{T-1},R_T,S_T$
   3. 初始化回报：$G \leftarrow 0$
   4. 对$t = T-1,T-2,\dots,0$执行以下操作：
      1. 更新回报：$G \leftarrow \gamma G + R_{t+1}$
      2. 更新策略：更新$\theta$以减小$-\frac{1}{b(A_t|S_t)}\gamma^tG_t\pi(A_t|S_t;\theta)$ （如$\theta_{t+1} \leftarrow \theta + \alpha \frac{1}{b(A_t|S_t)}\gamma^tG_t\nabla\pi(A_t|S_t;\theta)$）

重要性采样使得我们可以利用其他策略的样本来更新策略参数，但是可能会带来较大的偏差，算法稳定性比同策算法差。

