# Markov决策过程

## Markov决策过程模型

### 离散时间Markov决策过程

> 在离散时间智能体/环境接口中，智能体与环境交互的时刻为{0,1,2,3,...}。在时刻t，发生如下事情：
> * 智能体观测时间得$O_t$
> * 智能体根据观测环境决定做出动作$A_t$
> * 环境根据智能体动作，给予智能体奖励$R_{t+1}$，并进入下一步状态$S_{t+1}$

由此得：
* S：状态空间，状态取值的综合
* O：观测空间，观测取值的集合
* A：动作集合
  * 为了分析方便，往往采用一个包括所有可能动作的更大集合，使得每一步动作集合在数学上以同样的字母表示
* R：奖励空间，是实数集的子集

<u>离散时间不一定是间隔相同或间隔预先设定好的时间，这里的离散时间指标t只是表示决策和动作的指标</u>

【**轨道**】$S_0,O_0,A_0,R_1,S_1,O_1,A_1,R_2,...$

对于回合制任务，可能存在终止状态$s_{终止}$。
* 达到终止状态时，回合结束，不再有任何观测或动作
* *状态空间S中不包括终止状态*
* 强调含有终止状态的状态空间为$S^+$
* 回合制任务的轨道形式：$S_0,O_0,A_0,R_1,S_1,O_1,A_1,R_2,...,S_T = s_{终止}$
* 回合制度任务中一个回合的步数$T$：随机变量，随机过程中可以看做一个停时

> 部分不完全可观测问题：建模为部分可观测的Markov决策过程（POMDP）进行求解

【Markov决策过程模型】
* **概率**：在时间t，从状态$S_t = s$和动作$A_t = a$跳转到下一状态$S_{t+1} = s'$和动作$R_{t+1} = r$的概率为：$Pr[S_{t+1} = s',R_{t+1} = r|S_t = s,A_t = a]$
* **Markov性**：认为奖励$R_{t+1}$和下一状态$S_{t+1}$仅仅依赖于当前状态$S_t$和动作$A_t$，而不依赖于更早的状态和动作
  * 对状态的额外约束，状态必须含义可能对未来产生影响的所有过去信息

> 智能体/环境接口没有假设状态满足Markov性。Markov性是满足Markov决策过程的特点
> 可以从不满足Markov性的观测中构造满足Markov性的状态，或去学习Markov性

### 环境与动力

对于有限Markov过程，可定义：
* **动力**：$p(s',r|s,a) = Pr[S_{t+1} = s',R_{t+1} = r|S_t = s,A_t = a]$
* **状态转移概率**：$p(s'|s,a) = Pr[S_{t+1} = s'|S_t = s,A_t = a] = \sum_{r \in R} p(s',r|s,a)$
* **给定“状态-动作”的期望奖励**：$ r(s,a) = E[R_{t+1} | S_t = s,A_t = a] = \sum_{r \in R} r \sum_{s' \in S} p(s',r|s,a)$
* **给定“状态-动作-下一状态”的期望奖励**：$ r(s,a,s') = E[R_{t+1} | S_t = s,A_t = a,S_{t+1} = s'] = \sum_{r \in R} r \frac{p(s',r|s,a)}{p(s'|s,a)} $

注：$E[R_{t+1} | S_t = s,A_t = a,S_{t+1} = s'] = \sum_{r \in R} r p(r|s,s',a) = \sum_{r \in R} r \frac{p(r,s,s',a)}{p(s,s',a)} = \sum_{r \in R} r \frac{p(s',r|s,a)}{p(s'|s,a)}$

非有限Markov决策过程：定义时使用概率分布函数

### 智能体与策略

【**策略**】：从状态到动作的转移概率

对于有限Markov决策过程：$\pi: S \times A \to [0,1]$可定义为：$\pi(a|s) = Pr[A_t=a|S_t=s],s \in S, a\in A$

动作集为连续，则采用概率分布进行定义

【**确定性策略**】若某个策略$\pi$对状态集内任意状态，均有$a \in A$满足$\pi(a'|s) = 0, a' \ne a$，则称此策略为**确定性策略**。

### 奖励、回报和价值函数

【**回报**】
* 对于*回合制任务*，假设某一回合在第T步达到终止状态，则从步骤t以后的回报$G_t$可定义为未来奖励之和：$G_t = R_{t+1} + R_{t+2} +\dots +R_{T} $
* t为确定性变量，T为随机变量，因此在$G_t$定义中，不仅每一项是随机变量，而且含有的项数也是随机变量
* 对*连续性任务*，若只是简单求和，会趋于无穷大。由此引入**折扣**，此时定义回报：$G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} +\dots = \sum_{\tau = 0}^{+\infty}\gamma^\tau R_{t+\tau+1} $
  其中$\gamma \in [0,1]$
* 折扣因子决定了如何在最近的奖励和未来的奖励之间进行折中。未来$\tau$步后得到的1单位奖励相当于现在的$\gamma^\tau$单位奖励。
  * $\tau = 0$：只考虑眼前利益，无视未来利益——**贪心算法**
  * $\tau = 1$：未来1单位利益和眼前1单位利益同等重要
  一般设定$\gamma \in (0,1)$，若未来每一步奖励有界，则回报也有界。
* 也可为*连续性任务*定义**平均奖励**：$\bar{R} = \lim_{t \to \infty} E[\frac1t \sum_{\tau=1}^t R_\tau]$

【**状态价值函数**$v_\pi(s)$】
* 从状态s开始采用策略$\pi$的期望回报
* $v_\pi(s) = E_\pi[G_t|S_t = s]$

【**动作价值函数**$q_\pi(s,a)$】
* 状态s采用动作a后，采用策略$\pi$的期望回报
* $q_\pi(s,a) = E_\pi[G_t|S_t=s,A_t=a]$

对于终止状态：$v_pi(s_{终止}) = 0$；$q_pi(s_{终止},a) = 0$

## Bellman期望方程

【**策略评估**】试图求解给定策略的价值函数。Bellman方程常用于策略评估

【**状态价值和动作价值的互相表示**】
* 用t时刻的动作价值函数表示t时刻的状态价值函数：$v_{pi} (s) = \sum_a \pi(a|s)q_\pi(s,a), s \in S $，推导略。
  * 空心圆圈代表状态，实心圆圈代表状态动作对，则动作价值函数表示状态价值函数的过程可用**备份图**表示
* 用t+1时刻的状态价值函数表示t时刻的动作价值函数：$q_\pi(s,a) = \sum_{s',r}p(s',r|s,a)[r+\gamma v_\pi(s'')]= r(s,a) + \gamma\sum_{s'} p(s'|s,a)v_\pi(s'),s \in S, a \in A $

以上推导可灵活运用全概率公式、贝叶斯公式、概率乘法公式等内容灵活计算。

## 最优策略及其性质

定义一个偏序关系：对于两个策略$\pi$和$\pi'$，若对于任意$s \in S$都满足$v_\pi(s) \le v_{\pi'}(s)$，<u>则称策略$\pi$小于等于$\pi'$，记作$\pi \le \pi'$</u>

### 最优策略和最优价值函数

【**最优策略**】
* 对于一个动力，若状态空间$A(s)$是闭集，那么就存在一个策略$\pi_*$，使得所有的策略都小于等于这个策略，则称$\pi_*$为最优策略
* **最优价值函数**：最优策略的价值函数
  * **最优状态价值函数**：$v_* = \max_{\pi} v_\pi(s),s \in S$
  * **最优动作价值函数**：$q_*(s,a) = \max_{\pi} q_\pi(s,a)$

最优策略可能存在多个，但其价值函数是相同的，任取一个最优策略来考察也不是一般性。其中一种选取方法是选择这样的一种**确定性策略**：

$$\pi_*(s) = \mathop{\arg\max}_{a \in A} q_*(s,a),s \in S $$

若有多个动作a使得$q_*(s,a)$取得最大值，任选一个动作即可。

### Bellman最优方程

*最优价值函数的重要性质：Bellman最优方程*

最优策略的性质：
$$
    \pi_*(a|s) = \begin{cases} 
        1, &a = \mathop{\arg\max}_{a \in A} q_*(s,a')\\
        0,&others
    \end{cases}
$$

* 最优动作价值函数表示最优状态价值函数：$v_*(s) = \max_{a\in A} q_*(s,a)$ 
* 最优状态价值函数表示最优动作价值函数：$q_*(s,a) = r(s,a) + \gamma\sum_{s'}p(s'|s,a)v_*(s,a) = \sum_{s',r}p(s',r|s,a)[r+\gamma v_*(s')]$

由此得**Bellman最优方程**：
* 最优状态价值函数表示：$v_*(s) = \max_{a \in A} \left[ r(s,a) + \gamma\sum_{s'}p(s'|s,a)v_*(s,a) \right]$
* 最优动作价值函数表示：$q_*(s,a) = r(s,a) + \gamma\sum_{s'}p(s'|s,a)\max_{a'} q_*(s',a')$

用`sympy`求解。由于有max，为非线性方程，可通过分类化为线性方程

对于**给定数值**情况，常常将$v_*(s) = \max_{a\in A} q_*(s,a)$松弛为$v_*(s) \ge q_*(s,a)$，并消去q_*(s,a)以减少决策变量，得新的线性规划：
$$
    \begin{aligned}
        &\min &\sum_{s \in S} c(s)v(s)\\
        &\mathrm{over}&v(s),s \in S\\
        &\mathrm{s.t.}&v(s) \ge r(s,a) + \gamma\sum_{s'}p(s'|s,a)v_*(s,a)
    \end{aligned}
$$

$c(s)$为一组任意取值的整实数。Bellman最优方程的解显然会在线性规划的可行域内。此线性规划最优解满足Bellman最优方程。

### 用Bellman最优方程解求解最优策略

**最优策略**：对于每个状态$s$，总是选择让$q_*(s,a)$最大的动作$a$

**实际困难**：
* 难以列出Bellman最优方程
* 难以求解Bellman最优方程